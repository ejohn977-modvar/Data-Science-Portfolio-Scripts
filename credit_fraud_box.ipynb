{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3947079d26bd3fd81373f806838020dcdb7ab84b436ba6c81973f13c78869d8f"
   }
  },
  "interpreter": {
   "hash": "3947079d26bd3fd81373f806838020dcdb7ab84b436ba6c81973f13c78869d8f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use interactive python notebooks, so please adjust accordingly\n",
    "# if you copy and paste any of this code and are running a script instead.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ericj\\\\Documents\\\\credit_fraud\\\\creditcard.csv\")\n",
    "\n",
    "# Taking a look at the columns statistical summary.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# normalizing the class label counts to set the heights of the bars and the displays\n",
    "counts = df['Class'].value_counts(normalize=True) * 100\n",
    "counts[1] = counts[1] + 0.6 # ensuring that the bar for the minority class is actually visible\n",
    "\n",
    "# creating a bar plot\n",
    "plt.bar(x = df['Class'].unique(), height = counts, color = (0.8,0.5,0), tick_label = ['Legitimate', 'Fraudulent'])\n",
    "plt.title(label = 'Percentage of Transactions in each Class')\n",
    "plt.ylim(0,105) # expanding y-axis upward for better visual aesthetic\n",
    "\n",
    "# creating and roughly centering the labels\n",
    "plt.text(x = -0.05, y = 60, s = \"{0:5.3f}%\".format(counts[0]))\n",
    "plt.text(x = 0.95, y = 10, s = \"{0:4.3f}%\".format(counts[1] - 0.6)) # offsetting minority class bar height adjustment for text display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# using a copy of df I started with for correlation analysis\n",
    "df_for_correlation = df.copy(deep=True)\n",
    "\n",
    "# The Amount column covers a huge range. Converting to log-space.\n",
    "eps = 0.001 # 0 => 0.1Â¢\n",
    "df_for_correlation['Log_Amount'] = np.log(df_for_correlation.pop('Amount')+eps)\n",
    "\n",
    "# splitting data into features and labels\n",
    "y_all = df_for_correlation['Class']\n",
    "X_all = df_for_correlation.drop(columns= ['Time', 'Class'])\n",
    "\n",
    "# normalizing feature data to perform correlation analysis with Class labels\n",
    "# recall columns V1 - V28 were created  by PCA and so are already normalized\n",
    "# the only column that is not already normalized is the Log_Amount column \n",
    "# \"renormalizing\" the other columns will have no significant effect\n",
    "scaler = StandardScaler()\n",
    "X_np  = scaler.fit_transform(X_all)\n",
    "# easier to run and plot correlation analysis from pandas\n",
    "df_transformed = pd.DataFrame(data = X_np, columns = X_all.columns) \n",
    "# adding Class label back to dataset\n",
    "df_transformed['Class'] = y_all\n",
    "\n",
    "# creating the Pearson correlation matrix.\n",
    "corr_matrix = df_transformed.corr()\n",
    "corr_series = corr_matrix['Class'].abs().sort_values(ascending=False)\n",
    "seaborn.heatmap(corr_matrix)\n",
    "plt.show()\n",
    "\n",
    "# Taking the 14 features that are most highly correlated with class\n",
    "# The Class variable is included in this list at index 0! \n",
    "# Note we are not going to use Class to predict... Class\n",
    "important_features = corr_series[1:16].index.tolist()\n",
    "print(important_features)"
   ]
  },
  {
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import imblearn\n",
    "\n",
    "y = df['Class']\n",
    "X = df.drop(columns = ['Class', 'Time'])\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "\n",
    "# splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = seed)\n",
    "\n",
    "# I obtained the important features list by ordering the absolute values.\n",
    "# of a correlation analysis creating a list.\n",
    "# I have provided a hard-coded version of that list here.\n",
    "# important_features = ['V17', 'V14', 'V12', 'V10', 'V16', 'V3', 'V7', 'V11', 'V4', 'V18', 'V1', 'V9', 'V5', 'V2', 'V6']\n",
    "\n",
    "X_train = np.array(X_train[important_features])\n",
    "X_test = np.array(X_test[important_features])\n",
    "\n",
    "# basic shape inspections\n",
    "print('Training labels shape:', y_train.shape)\n",
    "print('Test labels shape:', y_test.shape)\n",
    "print('Training features shape:', X_train.shape)\n",
    "print('Test features shape:', X_test.shape)\n",
    "\n",
    "# creating imblearn resampling object\n",
    "# sampling strategy is the propotion of output\n",
    "# resampled data that is the minority class\n",
    "over_and_under_sample =  imblearn.combine.SMOTETomek(sampling_strategy = 1.0, n_jobs = -1, random_state = seed)\n",
    "X_train, y_train = over_and_under_sample.fit_resample(X_train, y_train)\n",
    "\n",
    "# checking under- and over-sample counts\n",
    "counter_train = Counter(y_train)\n",
    "counter_test = Counter(y_test)\n",
    "print(counter_train, counter_test)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # declaring an out-of-box-XGBoost classifier\n",
    "# model = XGBClassifier()\n",
    "# print(model)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# declaring an XGBoost classifier\n",
    "model = XGBClassifier(learning_rate = 0.01, n_estimators = 600, max_depth = 5, objective = 'binary:logistic', eval_metric = 'logloss', base_score = 0.95, gamma = 1.55, reg_lambda = 9, random_state = seed)\n",
    "print(model)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate our predictions\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100)\n",
    "print(confusion_matrix)\n",
    "\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "print(\"precision: %.2f%%\" % (precision * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Legal', 'Fraud']\n",
    "\n",
    "print(classification_report(y_true = y_test, y_pred = predictions, target_names = target_names, \n",
    "digits = 3))\n",
    "\n",
    "# evaluate the predictions\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100)\n",
    "print(confusion_matrix, \"\\n \\n\", \"Recall: %.2f%%\" % (recall * 100), \"\\n\", \"Accuracy: %.2f%%\" % (accuracy * 100),\n",
    "\"\\n\", \"precision: %.2f%%\" % (precision * 100))"
   ]
  }
 ]
}